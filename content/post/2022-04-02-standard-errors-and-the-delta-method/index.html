---
title: Standard Errors and the Delta Method
author: R package build
date: '2022-04-02'
slug: []
categories:
  - R
tags:
  - R
subtitle: ''
summary: ''
authors: []
lastmod: '2022-04-01T16:45:20-07:00'
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>A friend recently asked me a question about the delta method, which sent me briefly into a cold sweat trying to remember a concept from my first year methods sequence. In the spirit of notes to myself, here’s a post explaining what it is, why you might want to use it, and how to calculate it with R.</p>
</div>
<div id="what-is-the-delta-method" class="section level2">
<h2>What is the Delta Method?</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Delta_method">Delta method</a> is a result concerning the asymptotic behavior of functions over a random variable. Read asymptotics as “what happens to the thing I’m estimating as my sample gets big?”<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> The usual link between random variables and a sample is that each observation in a sample that is independent and identically distributed is an observation of a random variable.</p>
<p>The most famous theorem I know, the Central Limit Theorem, implies that when <em>n</em> gets large, the distribution of the difference between the sample mean and the population mean <span class="math inline">\(\mu\)</span> when multiplied by <span class="math inline">\(\sqrt{n}\)</span> converges in distribution to a normal distribution <span class="math inline">\(N(0,\sigma^2)\)</span>.</p>
<p>Like the Central Limit Theorem, the Delta method tells us something about the asymptotic behavior of functions of random variables. Specifically, we can approximate the asymptotic behavior of functions over the random variable.</p>
</div>
<div id="why-would-i-use-it" class="section level2">
<h2>Why would I use it?</h2>
<p>One reason as this <a href="https://www.jepusto.com/delta-method-and-2sls-ses/">nice post</a> by James Pustejovsky points out is that you already do use it if you’ve ever estimated anything by 2SLS. Anytime you have a ratio estimator, the delta method is a way of construction standard errors. To be even more general, the delta method is useful in any situation where we want to form confidence intervals for nonlinear functions of parameters.</p>
<p>Suppose have a smooth function <span class="math inline">\(g()\)</span> that has parameter <span class="math inline">\(\beta\)</span> and we have an estimate <span class="math inline">\(b\)</span> from some consistent, normally distributed estimator for the parameter, then that estimator converges to an asymptotically normal distribution. Via Taylor’s Theorem since <span class="math inline">\(g()\)</span> is continuous and derivable up to some <span class="math inline">\(k^{th}\)</span> derivative (<span class="math inline">\(k \geq 2\)</span>), then at our function evaluated at <span class="math inline">\(b\)</span> is:</p>
<p><span class="math display">\[
g(b) \approx g(\beta) + \nabla g(\beta)&#39;(b-\beta)
\]</span></p>
<p>also known as the mean value expansion. We can subtract <span class="math inline">\(g(\beta)\)</span> from each side to get:</p>
<p><span class="math display">\[
g(b) - g(\beta) \approx \nabla g(\beta)&#39;(b-\beta)
\]</span></p>
<p>We’ll multiply both sides by <span class="math inline">\(\sqrt{n}\)</span> because we can and because it gives us the following nice property.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Since <span class="math inline">\(\beta\)</span> is a constant and <span class="math inline">\(b\)</span> is a consistent estimator for it,</p>
<p><span class="math display">\[
\sqrt{n}(g(b) -g(\beta))
\]</span></p>
<p>converges in distribution to</p>
<p><span class="math display">\[
N(0, \nabla g(\beta)^T*\sum_b * \nabla g(\beta))
\]</span></p>
<p>where the middle term represents the variance covariance matrix. That <span class="math inline">\(\nabla\)</span> is the symbol for the gradient of the function, which is the “vector differential operator” and when applied to a function on a multi-dimensional domain is the vector of partial derivatives, which we evaluate at a given point.</p>
<p>The appeal of the delta method is that we get an analytic approximation of a function’s behavior by using asymptotic properties, which can be computationally pretty simple. Depending on what we’re calculated, it might actually secretly be something we’ve already been doing under a different name. Of course, we could also just bootstrap the thing, but that’s less fun now isn’t it.</p>
</div>
<div id="i-learn-best-by-example.-show-me-a-worked-example" class="section level2">
<h2>I learn best by example. Show me a worked example</h2>
<p>You bet. Consider Testa, Williams, Britzman, and Hibbing’s “Getting the Message? Choice Self-Selection, and the Efficacy of Social Movement Arguments” published in JEPS.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> The purpose of this paper is to show that designs that incorporate choice can be extended to randomize conditional on subjects’ treatment choices to answer questions of interest while preserving statistical power. Pretty cool. The authors apply the design to study how the gender of messengers for the #MeToo social movement conditions 1) who receives the message and 2) how they respond. The Estimand of interest in one part of their design is an Average Choice-Specific Treatment Effect (ACTE), which is found by taking a weighted average of an outcome in the selection condition and the average of those in the experimental control, and then dividing this estimate by the proportion of people seeking treatment.</p>
<p>What does that mean for the delta method? Well it means that the authors have a ratio estimate!</p>
<p><img src="images/Screen%20Shot%202022-04-01%20at%208.34.14%20PM.png" /></p>
<p>The authors find that in their design the estimate of the ACTE for the full sample on specific support for the #MeToo movement is 0.19 with a confidence interval of <span class="math display">\[-0.01, 0.39\]</span>.</p>
<p>How did they get those numbers? Trust me when I tell you that this is an interesting paper and you should read it. The extremely short version is that they estimate a function <span class="math inline">\(g(x,y) = \frac{x}{y}\)</span>. To get a confidence interval via the delta method, we need to find the standard deviation of the asymptotic distribution of this estimator, which is equivalent to the square root of the variance of the distribution. From above, we know that we need get three terms <span class="math inline">\(\nabla g(\beta)\)</span>, the variance covariance matrix, and <span class="math inline">\(\nabla g(\beta)&#39;\)</span>.</p>
<p>Let’s go in order. The gradient is just the vector of all the partial derivatives of <span class="math inline">\(g(x,y)\)</span> with respect to each variable.</p>
<p><span class="math display">\[
\nabla g(\beta) =\begin{bmatrix}
\frac{1}{y} \\
\frac{-x}{y^2}
\end{bmatrix}
\]</span></p>
<p>Plugging in the authors’ estimates of <em>x</em> and <em>y</em> and evaluate each of the partial derivatives at our estimates we have:</p>
<p><span class="math display">\[
\nabla g(b) =\begin{bmatrix}
\frac{1}{.813} \approx 1.23 \\
\frac{-.1532}{.813^2} \approx -0.23
\end{bmatrix}
\]</span></p>
<p>The variance covariance matrix here is just the variances of x and y on the diagonals because due to the randomization the covariance terms are 0. Once again plugging in the authors’ estimates of these terms:</p>
<p><span class="math display">\[
\sum_b =\begin{bmatrix}
\sigma_x^2 \approx .07 &amp; 0 \\\\
0 &amp; \sigma_y^2 \approx -0.0002
\end{bmatrix}
\]</span></p>
<p>Great, now we have all three terms (since one is just the transpose of something we know). Now we do some matrix multiplication.</p>
<p><span class="math display">\[
\begin{align}
V[b] &amp;= \nabla g(b)^T\sum_b \nabla g(b) \\\\
V[b] &amp;= .010 \\\\
SE[b] &amp;= \sqrt(V[b]) \\\\
SE[b] &amp;= .102
\end{align}
\]</span></p>
<p>Being lazy and using the rule of thumb that a 95% CI critical value is 1.96:</p>
<p><span class="math display">\[
\begin{aligned}
\theta &amp;\pm 1.96*.102 \\\\
\theta &amp;\pm \approx .2 \\\\
.19 &amp;\pm \approx .2
\end{aligned}
\]</span></p>
<p>which works out to the the confidence interval in the paper.</p>
</div>
<div id="i-have-to-do-this-in-r.-show-me-a-worked-example" class="section level2">
<h2>I have to do this in R. Show me a worked example</h2>
<p>There’s actually a lot going on to get the weighted average estimates, but I’ll put that aside for a separate post about weighted variances. For now, note that we can note that a general property is that the gradient is a special case of the Jacobian when the function is a scalar.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The function that I know of to calculate numerical derivatives in R is <code>numDeriv::jacobian</code>. This function requires us to pass a function argument, so I first make a function called ratio for this purpose.</p>
<pre class="r"><code>## function that we seek to estimate 
## pass this to the jacobian function
## which calculates the gradient 
ratio &lt;- function(x){
  x[1]/x[2]
}


## this function by default will output this in 
## transpose form, so take the transpose to match 
## discussion above
grad_g &lt;- t(numDeriv::jacobian(func = ratio, 
                               c(.153, .853)))

## Variance covariance matrix 
vcov &lt;- diag(c(0.07,0.0002)^2)

## Estimate the standard error
se_b &lt;- sqrt(t(grad_g) %*% vcov %*%grad_g)</code></pre>
<p>Now all that is left to do is calculate the confidence interval:<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<pre class="r"><code>estimate = .19 
lwr_b = estimate - 1.96*se_b 
upp_b = estimate + 1.96*se_b</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">estimate</td>
<td align="right">0.19</td>
</tr>
<tr class="even">
<td align="left">lwr_b</td>
<td align="right">-0.01</td>
</tr>
<tr class="odd">
<td align="left">upp_b</td>
<td align="right">0.39</td>
</tr>
</tbody>
</table>
<p>Voila. Confidence Intervals.</p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>With extreme apologies to Wooldridge (2010).<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Cameron and Trevedi (2005, p. 231); Wooldridge (2010, p. 45)<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://paultesta.org/publication/testa-2020-a/testa-2020-a.pdf" class="uri">https://paultesta.org/publication/testa-2020-a/testa-2020-a.pdf</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Thanks <a href="https://math.stackexchange.com/questions/1519367/difference-between-gradient-and-jacobian">Stack Exchange</a>.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Replication note. The actual calculations in the paper are far more precise. Running this code exactly will give you different confidence intervals due to this fact. This is not a problem for the original paper, but rather is due to purposefully simplifying on my end.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
