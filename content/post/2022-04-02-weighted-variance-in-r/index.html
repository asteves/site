---
title: Weighted Standard Errors in R
author: R package build
date: '2022-04-02'
slug: []
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2022-04-02T12:12:55-07:00'
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
editor_options: 
  markdown: 
    wrap: 72
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>I was recently working on a problem that required me to take a weighted
standard error of a difference in means function. While R does have a
<code>weighted.mean()</code> function, it does not have a similar function for
weighted variance. That sent me down a bit of a rabbit hole to figure
out how to calculate the weighted standard error of the mean. This post
is a short introduction to weighted averages and weighted variances, and
also presents the correct way to calculate the standard error of a
weighted mean. I also include R functions as a reference to myself and
anyone else who stumbles upon this issue in the future.</p>
<div id="weighted-averages" class="section level2">
<h2>Weighted Averages</h2>
<p>Weighted averages show up often in data analysis. There are two reasons
for this, one conceptual and one practical. The conceptual reason is
that just about every estimand of interest is itself an average. Why is
this? That pesky pesky fundamental problem of causal inference. Since
causal effects are defined by comparing potential outcomes, and only one
outcome can ever be observed for an individual, the problem of causal
inference is a missing data problem.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> As a consequence, we require
multiple units to make any inference, and have to make do with an
average instead of individual causal effects. ATE? <em>Average!</em> CATE?
<em>Conditional Average</em>! LATE? <em>Local Average</em>! Everything is an average.</p>
<p>Normally, the set up for causal inference tends to make an implicit
assumption that the causal effect for every unit is constant.
Practically, we probably should never believe that to be true for
anything interesting. The treatment effect should be heterogeneous,
which means that the estimand of interest is going to be a weighted
average.</p>
<p>The practical reason that weighted averages show up often is that the
definition of an average itself is a definition of a weighted sum.
Normally it is introduced like this:</p>
<p><span class="math display">\[
\mu = \frac{1}{n}\sum_i^n x_i
\]</span></p>
<p>We can generalize that definition to deal with weights.</p>
<p><span class="math display">\[
\mu = \frac{\sum_i^nw_ix_i}{\sum_i^nw_i}
\]</span></p>
<p>where the <span class="math inline">\(w_i\)</span> indicates the weight for each unit. If the weights are
all equal <span class="math inline">\(w_i = \frac{1}{n}\)</span>, and that formula reduces to the usual
definition of an average.</p>
<p>In R by hand</p>
<pre class="r"><code>weighted.average &lt;- function(x, w){
    ## Sum of the weights 
    sum.w &lt;- sum(w, na.rm = T)
    ## Sum of the weighted $x_i$ 
    xw &lt;- sum(w*x, na.rm = T)
    
    ## Return the weighted average 
    return(xw/sum.w)
}</code></pre>
<p>In action, we can see that with equal weights this reduces to the usual
mean function.</p>
<pre class="r"><code>x &lt;- c(1,2,3,4)
w &lt;- c(1,1,1,1)

## test against R&#39;s built in mean function
weighted.average(x, w) == mean(x)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>When the weights are unequal, this returns what we expect.</p>
<pre class="r"><code>x2 &lt;- c(1,2,3,4)
w2 &lt;- c(2,1,2,3)

## Test against R&#39;s built in weighted.mean function
weighted.average(x2, w2) == weighted.mean(x2, w2)</code></pre>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="weighted-variance" class="section level2">
<h2>Weighted Variance</h2>
<p>So far so good, and at least in R there are default functions to do
these calculations for us.</p>
<p>The trouble arises when we are interested in the uncertainty of our
calculation, as would occur in a sample of data. We would like to know
what is the variance of a weighted average. To find our formula we can
derive this from the definition of variance.</p>
<p>Define <span class="math inline">\(W = \sum_i^n w_i\)</span> and <span class="math inline">\(\bar{x} = \frac{1}{W}\sum_i^nw_ix_i\)</span>
which is the weighted average. Then:</p>
<p><span class="math display">\[\begin{aligned}
V[\bar{x}] &amp;= V[\frac{1}{W}\sum_i^nw_ix_i]\\
V[\bar{x}] &amp;= \sum_i^nV[\frac{w_i}{W}x_i] \\
V[\bar{x}] &amp;= \sum_i^n\left(\frac{w_i}{W}\right)^2V[x_i] \\
V[\bar{x}] &amp;= V[x]\sum_i^n\left(\frac{w_i}{W}\right)^2
\end{aligned}\]</span></p>
<p>What’s going on in the derivation. Line 1 is simply the definition of
variance. In Line 2 we use a property that the variance of independent
sums is equal to the sum of the variances. Line 3 is an application of
the fact that <span class="math inline">\(V[aX] = a^2V[X]\)</span>. Line 4 is due to the fact that sum of
the variances of <span class="math inline">\(x_i\)</span> is just <span class="math inline">\(V[x]\)</span> which can pulled out of the
summation. Note that if all the weights are equal, the weights fraction
reduces to <span class="math inline">\(\frac{1}{n}\)</span>. To see this, consider the example above with
equal weights. Each <span class="math inline">\(\frac{w_i}{W} = \frac{1}{n}\)</span> so the total sum is
<span class="math inline">\(n\frac{1}{n^2}\)</span> which reduces to <span class="math inline">\(\frac{1}{n}\)</span> after cancelling terms.</p>
<p>Normally when we consider a weighted average, we are thinking of the
case where we want to give more weight to some points rather than
others. For example, if we take a survey of 100 people and weight by
self-identified race, some points will get larger weight.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> In my
work, this is the dominant weighting scenario to worry about. If we use
the usual standard error of the mean formula described in footnote 2 in
this case we will end up with <a href="https://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf">a very unstable estimate of the standard
error of the mean as weights become
uneven</a>.</p>
<pre class="r"><code>## Function to calculate standard error with weights 
## Incorrect in case where weights imply meaning and not
## uncertainty
bad_se &lt;- function(x, w){
    n = length(x)
    numerator = sum(w*(x-weighted.average(x,w))^2)
    denominator = sum(w)
    correction = n/(n-1)
    var_x = (correction * (numerator/denominator))/n
    return(sqrt(var_x))
}</code></pre>
<p>To see why, consider a case where we place a massive amount of weight on
one observation in a sequence. Here we generate every integer from 1 to
100 and see what happens when we use the usual standard error.</p>
<pre class="r"><code>x4 &lt;- seq(1,100,1)
w4 &lt;- c(rep(1,99),10000)
avg = round(weighted.average(x4,w4),2)
se_avg = round(bad_se(x4,w4),2)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">average</td>
<td align="right">99.51</td>
</tr>
<tr class="even">
<td align="left">se</td>
<td align="right">0.57</td>
</tr>
</tbody>
</table>
<p>The average is close to but not quite 100, which makes sense because the
last value of x has a massive weight compared to every other value. The
standard error though is functionally zero because the last weight is
dominating every other value. What is happening here is that the
<em>effective degrees of freedom</em> is very small. The last point has such a
large weight that it dominates all the other points, and we are not
correcting for this switch.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Instead of doing what we expect, become
larger because the weights are uneven, this estimate of the standard
error of the mean goes to zero.</p>
<p>We should thus correct for this loss of degrees of freedom by getting
the effective sample size and use it in our standard error
calculation.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p><span class="math display">\[
n_{eff} = \frac{(\sum_i^nw_i)^2}{\sum_i^nw_i^2}
\]</span></p>
<p>Note that when all the weights are equal, the effective sample size is
identical to the nominal sample size because the numerator is <span class="math inline">\(n^2\)</span> and
the denominator is <span class="math inline">\(n\)</span>. However, if the weights are dominated by a
single point, then the effective sample size will converge to 1. To see
this imagine that we have two points, and the second is 100 times as
important as the first.</p>
<p><span class="math display">\[
n = \frac{(1+100)^2}{(1+10000)} \\
n = \frac{101^2}{10001} \\
n = 1.02
\]</span></p>
<p>By using the effective n, we will get an unbiased estimate of the
standard error. Here’s how to write this function in R.</p>
<pre class="r"><code>weighted.se.mean &lt;- function(x, w, na.rm = T){
    ## Remove NAs 
    if (na.rm) {
      i &lt;- !is.na(x)
        w &lt;- w[i]
        x &lt;- x[i]
    }
    
    ## Calculate effective N and correction factor
    n_eff &lt;- (sum(w))^2/(sum(w^2))
    correction = n_eff/(n_eff-1)
    
    ## Get weighted variance 
    numerator = sum(w*(x-weighted.average(x,w))^2)
    denominator = sum(w)
    
    ## get weighted standard error of the mean 
    se_x = sqrt((correction * (numerator/denominator))/n_eff)
    return(se_x)
}</code></pre>
<p>We can now apply it to our example case.</p>
<pre class="r"><code>x4 &lt;- seq(1,100,1)
w4 &lt;- c(rep(1,99),10000)
correct_avg = round(weighted.average(x4,w4),3)
correct_se = round(weighted.se.mean(x4,w4),3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">average</td>
<td align="right">99.510</td>
</tr>
<tr class="even">
<td align="left">correct_se</td>
<td align="right">40.274</td>
</tr>
</tbody>
</table>
<p>Now we get an estimate of the standard error that is far more in line
with our expectation.</p>
<p>As a coda, in case you do not want to roll your own function there is a
weighted variance function in the <code>Hmisc</code> R package. This could be used
with the appropriate corrections to get the same answer.</p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Rubin 1979<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>This contrasts from a situation where all of our respondents have
equal weight but some of the points have larger uncertainties than
others. I’m not focusing on that case here. In that case, the
calculation of the standard error is done by the usual approach but
including weights in the formula.</p>
<p><span class="math display">\[
SE[\bar{x}] = \frac{\sqrt{\frac{n}{n-1}\frac{\sum_i^n w_i(x_i -\bar{x})^2}{\sum_i^n w_i}}}{\sqrt{n}}
\]</span></p>
<p>If the weights are all equal this reduces to the usual definition of
the standard error. If they are unequal but inversely proportional
to the individual uncertainties then this is unbiased.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Incidentally, on the topic of everything being an average, a
similar thing happens in a regression. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12185">Aronow and Samii
(2015)</a>
show that a multiple regression yields a weighted estimate of our
coefficient of interest where observations with less predictable
treatment get more weight. Observations with zero weights contribute
nothing to our estimates. As a result, the effective sample in an
OLS regression without an unconfounded assignment mechanism may be
much smaller than the nominal <em>n.</em><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://en.wikipedia.org/wiki/Effective_sample_size" class="uri">https://en.wikipedia.org/wiki/Effective_sample_size</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
